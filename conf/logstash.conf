input {
  stdin {
    type => "syslog"
  }
}

filter {
  if [type] == "syslog" {
    grok {
      match => {"message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}"}
      # match => { "publishData" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      #add_field => [ "streamId", "system.log.1.0.0" ]
      #add_field => {"testMap" => {"test" => "%{host}"}}
    }
    syslog_pri {}
    date {
      match => ["syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss"]
    }
  }
  #dynamically configure the streamId to match the corresponding stream
  if [type] == "syslog" {
    mutate {add_field => {"streamId" => "system.log.1.0.0"}}
  }else if [type] == "apache" {
    mutate {add_field => {"streamId" => "apache.log.1.0.0"}}
  }else{
  mutate {add_field => {"streamId" => "test.log.1.0.0"}}
}
}

output {

  dasConnector {
    http_method => "post"
    url => "http://requestb.in/1enbj1e1"
    content_type => "json"

    # mandatory fields for the stream definition
    # DO NOT CHANGE THIS AFTER THE STREAM IS DEFINED
    payloadData =>  ["type","streamId"]

    # Arbitrary values field, Specify which fields are to be indexed along with it's type
    #these will be stored and indexed
    #ex : arbitraryValues => { "fieldName" => "data_type"}
    arbitraryValues => {"syslog_program" => "String" "syslog_message" => "String"}

    correlationData => "correlation data"
    metaData => "test"
  }

  stdout {
    codec => "rubydebug"
  }
}