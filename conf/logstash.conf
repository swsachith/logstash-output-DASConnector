input {
  stdin {
    type => "syslog"
  }
  #file {
  #  type => "clientlog"
  #  path => "/home/sachith/work/temp/LogAnalytics_poc/CCEUMWLSQA01MS1_eum.log"
  #  start_position => "beginning"
  #  sincedb_path => "/dev/null"
  #}
  stdin {
    type => "clientlog"
  }
}

filter {

  if [type] == "syslog" {
    grok {
      match => {"message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}"}
      # match => { "publishData" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "streamId", "system.log.1.0.0" ]
      #add_field => {"testMap" => {"test" => "%{host}"}}
    }
    syslog_pri {}
    date {
      match => ["syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss"]
    }
  }
  if [type] == "clientlog" {
    grok {
      match => {"message" => "%{DATA:data}"}

    }

  }
}

output {


stdout {
  codec => "rubydebug"
}
  dasConnector {
    http_method => "post"
    url => "http://localhost:9763"
    content_type => "json"
    streamName => "logs"
    streamVersion => "1.0.0"

    # mandatory fields for the stream definition
    # DO NOT CHANGE THIS AFTER THE STREAM IS DEFINED
    #payloadFields => {"type" => "String" "streamId" => "String"}
    payloadFields => {"type" => "String" }

    # Arbitrary values field, Specify which fields are to be indexed along with it's type
    #these will be stored and indexed
    #ex : arbitraryValues => { "fieldName" => "data_type"}
    #arbitraryValues => {"syslog_program" => "String" "syslog_message" => "String"}
    arbitraryValues => {"host" =>"String" "syslog_program" => "String"}

    correlationData => {"activity_id" => "facet"}
    metaData => {}

    schemaDefinition => {
        "tableName" => "logs"
        "schemaURL" => "http://localhost:9763"
    }
  }
}